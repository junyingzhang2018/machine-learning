{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex6_spam Spam Classification\n",
    "\n",
    "We will use SVM to build a spam classifier. We will train a classifier to classify whether a given email x is spam (y=1) or non-spam (y=0). We need to convert each email into a feature vector x. For the purpose of the exercise, we will only use the email body (excluding the email header)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "from scipy.optimize import minimize\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Email Preprocessing\n",
    "\n",
    "To use an SVM to classify emails into Spam v.s. Non-Spam, you first need to convert each email into a vector of features. In this part, you will implement the preprocessing steps for each email. You should complete the code in processEmail.m to produce a word indices vector for a given email.\n",
    "\n",
    "1. Lower-casing\n",
    "\n",
    "2. Stripping HTML\n",
    "\n",
    "3. Normalizing URLs\n",
    "\n",
    "4. Normalizing Email address\n",
    "\n",
    "5. Normalizing Numbers\n",
    "\n",
    "6. Normalizing dollars\n",
    "\n",
    "7. Word stemming\n",
    "\n",
    "8. Removal of non-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readFile(filename):\n",
    "    #READFILE reads a file and returns its entire contents \n",
    "    #   file_contents = READFILE(filename) reads a file and returns its entire\n",
    "    #   contents in file_contents\n",
    "    #\n",
    "\n",
    "    # Load File\n",
    "    try:\n",
    "        with open(filename, 'r') as openFile:\n",
    "            file_contents = openFile.read()\n",
    "    except:\n",
    "        file_contents = ''\n",
    "        print('Unable to open {:s}'.format(filename))\n",
    "\n",
    "    return file_contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"> Anyone knows how much it costs to host a web portal ?\\n>\\nWell, it depends on how many visitors you're expecting.\\nThis can be anywhere from less than 10 bucks a month to a couple of $100. \\nYou should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \\nif youre running something big..\\n\\nTo unsubscribe yourself from this mailing list, send an email to:\\ngroupname-unsubscribe@egroups.com\\n\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract Features\n",
    "email=readFile('emailSample1.txt');\n",
    "print(type(email))\n",
    "email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getVocabList():\n",
    "    #GETVOCABLIST reads the fixed vocabulary list in vocab.txt and returns a\n",
    "    #cell array of the words\n",
    "    #   vocabList = GETVOCABLIST() reads the fixed vocabulary list in vocab.txt \n",
    "    #   and returns a cell array of the words in vocabList.\n",
    "\n",
    "\n",
    "    ## Read the fixed vocabulary list\n",
    "    with open('vocab.txt', 'r') as vocabFile:\n",
    "\n",
    "        # Store all dictionary words in dictionary vocabList\n",
    "        vocabList = {}\n",
    "        for line in vocabFile.readlines():\n",
    "            i, word = line.split()\n",
    "            vocabList[word] = int(i)\n",
    "\n",
    "    return vocabList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabList=getVocabList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processEmail(email_contents):\n",
    "    import re\n",
    "    from nltk import PorterStemmer\n",
    "    \n",
    "#PROCESSEMAIL preprocesses a the body of an email and\n",
    "#returns a list of word_indices \n",
    "#   word_indices = PROCESSEMAIL(email_contents) preprocesses \n",
    "#   the body of an email and returns a list of indices of the \n",
    "#   words contained in the email. \n",
    "\n",
    "# Load Vocabulary\n",
    "    vocabList = getVocabList();\n",
    "\n",
    "# Init return value\n",
    "    word_indices = [];\n",
    "\n",
    "# ========================== Preprocess Email ===========================\n",
    "\n",
    "# Find the Headers ( \\n\\n and remove )\n",
    "# Uncomment the following lines if you are working with raw emails with the\n",
    "# full headers\n",
    "\n",
    "# hdrstart = strfind(email_contents, ([char(10) char(10)]));\n",
    "# email_contents = email_contents(hdrstart(1):end);\n",
    "\n",
    "# Lower case\n",
    "    email_contents = email_contents.lower();\n",
    "\n",
    "# Strip all HTML\n",
    "# Looks for any expression that starts with < and ends with > and replace\n",
    "# and does not have any < or > in the tag it with a space\n",
    "    email_contents = re.sub('<[^<>]+>', ' ', email_contents);\n",
    "\n",
    "# Handle Numbers\n",
    "# Look for one or more characters between 0-9\n",
    "    email_contents = re.sub('[0-9]+', 'number',email_contents);\n",
    "\n",
    "# Handle URLS\n",
    "# Look for strings starting with http:// or https://\n",
    "    email_contents = re.sub('(http|https)://[^\\s]*', 'httpaddr', email_contents);\n",
    "                           \n",
    "# Handle Email Addresses\n",
    "# Look for strings with @ in the middle\n",
    "    email_contents = re.sub('[^\\s]+@[^\\s]+', 'emailaddr', email_contents);\n",
    "\n",
    "# Handle $ sign\n",
    "    email_contents = re.sub('[$]+', 'dollar', email_contents);\n",
    "\n",
    "# ========================== Tokenize Email ===========================\n",
    "\n",
    "# Output the email to screen as well\n",
    "    print('\\n==== Processed Email ====\\n\\n');\n",
    "\n",
    "# Process file\n",
    "    l = 0;\n",
    "    # Slightly different order from matlab version\n",
    "\n",
    "    # Split and also get rid of any punctuation\n",
    "    # regex may need further debugging...\n",
    "    email_contents = re.split(r'[@$/#.-:&\\*\\+=\\[\\]?!(){},\\'\\'\\\">_<;%\\s\\n\\r\\t]+', email_contents)\n",
    "\n",
    "    for token in email_contents:\n",
    "\n",
    "        # Remove any non alphanumeric characters\n",
    "        token = re.sub('[^a-zA-Z0-9]', '', token)\n",
    "\n",
    "        # Stem the word \n",
    "\n",
    "        token = PorterStemmer().stem(token.strip())\n",
    "\n",
    "        # Skip the word if it is too short\n",
    "        if len(token) < 1:\n",
    "           continue\n",
    "  \n",
    "\n",
    "    # Look up the word in the dictionary and add to word_indices if\n",
    "    # found\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Fill in this function to add the index of str to\n",
    "    #               word_indices if it is in the vocabulary. At this point\n",
    "    #               of the code, you have a stemmed word from the email in\n",
    "    #               the variable str. You should look up str in the\n",
    "    #               vocabulary list (vocabList). If a match exists, you\n",
    "    #               should add the index of the word to the word_indices\n",
    "    #               vector. Concretely, if str = 'action', then you should\n",
    "    #               look up the vocabulary list to find where in vocabList\n",
    "    #               'action' appears. For example, if vocabList{18} =\n",
    "    #               'action', then, you should add 18 to the word_indices \n",
    "    #               vector (e.g., word_indices = [word_indices ; 18]; ).\n",
    "    # \n",
    "    # Note: vocabList{idx} returns a the word with index idx in the\n",
    "    #       vocabulary list.\n",
    "    # \n",
    "    # Note: You can use strcmp(str1, str2) to compare two strings (str1 and\n",
    "    #       str2). It will return 1 only if the two strings are equivalent.\n",
    "    #\n",
    "        idx = vocabList[token] if token in vocabList else 0\n",
    "\n",
    "        # only add entries which are in vocabList\n",
    "        #   i.e. those with ind ~= 0, \n",
    "        #        given that ind is assigned 0 if str is not found in vocabList\n",
    "        if idx > 0:\n",
    "            word_indices.append(idx)\n",
    "\n",
    "        # =============================================================\n",
    "\n",
    "\n",
    "        # Print to screen, ensuring that the output lines are not too long\n",
    "        if l + len(token) + 1 > 78:\n",
    "            print(\"\")\n",
    "            l = 0\n",
    "        print('{:s}'.format(token)),\n",
    "        l = l + len(token) + 1\n",
    "\n",
    "    # Print footer\n",
    "    print('\\n\\n=========================\\n')\n",
    "\n",
    "    return word_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemmers remove morphological affixes from words, leaving only the word stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Processed Email ====\n",
      "\n",
      "\n",
      "anyon\n",
      "know\n",
      "how\n",
      "much\n",
      "it\n",
      "cost\n",
      "to\n",
      "host\n",
      "a\n",
      "web\n",
      "portal\n",
      "well\n",
      "it\n",
      "depend\n",
      "on\n",
      "how\n",
      "mani\n",
      "\n",
      "visitor\n",
      "you\n",
      "re\n",
      "expect\n",
      "thi\n",
      "can\n",
      "be\n",
      "anywher\n",
      "from\n",
      "less\n",
      "than\n",
      "number\n",
      "buck\n",
      "a\n",
      "month\n",
      "\n",
      "to\n",
      "a\n",
      "coupl\n",
      "of\n",
      "dollarnumb\n",
      "you\n",
      "should\n",
      "checkout\n",
      "httpaddr\n",
      "or\n",
      "perhap\n",
      "amazon\n",
      "ecnumb\n",
      "\n",
      "if\n",
      "your\n",
      "run\n",
      "someth\n",
      "big\n",
      "to\n",
      "unsubscrib\n",
      "yourself\n",
      "from\n",
      "thi\n",
      "mail\n",
      "list\n",
      "send\n",
      "an\n",
      "\n",
      "email\n",
      "to\n",
      "emailaddr\n",
      "\n",
      "\n",
      "=========================\n",
      "\n",
      "Word Indices: \n",
      "\n",
      "[86, 916, 794, 1077, 883, 370, 1699, 790, 1822, 1831, 883, 431, 1171, 794, 1002, 1893, 1364, 592, 1676, 238, 162, 89, 688, 945, 1663, 1120, 1062, 1699, 375, 1162, 479, 1893, 1510, 799, 1182, 1237, 810, 1895, 1440, 1547, 181, 1699, 1758, 1896, 688, 1676, 992, 961, 1477, 71, 530, 1699, 531]\n"
     ]
    }
   ],
   "source": [
    "word_indices  = processEmail(email);\n",
    "\n",
    "# Print Stats\n",
    "print('Word Indices: \\n');\n",
    "print( word_indices);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Extraction\n",
    "\n",
    "We will convert each email into a vector of feature in R^n. You should complete the code in emailFeature.m to produce a feature vector for a given email. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1899"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabList = getVocabList();\n",
    "len(vocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emailFeatures(word_indices):\n",
    "#EMAILFEATURES takes in a word_indices vector and produces a feature vector\n",
    "#from the word indices\n",
    "#   x = EMAILFEATURES(word_indices) takes in a word_indices vector and \n",
    "#   produces a feature vector from the word indices. \n",
    "\n",
    "# Total number of words in the dictionary\n",
    "    n = 1899;\n",
    "\n",
    "# You need to return the following variables correctly.\n",
    "    x = np.zeros((n, 1));\n",
    "    vocabList = getVocabList();\n",
    "    for i in word_indices:\n",
    "        if i in vocabList.values():\n",
    "            x[i,0]=1\n",
    "    return x\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "# Instructions: Fill in this function to return a feature vector for the\n",
    "#               given email (word_indices). To help make it easier to \n",
    "#               process the emails, we have have already pre-processed each\n",
    "#               email and converted each word in the email into an index in\n",
    "#               a fixed dictionary (of 1899 words). The variable\n",
    "#               word_indices contains the list of indices of the words\n",
    "#               which occur in one email.\n",
    "# \n",
    "#               Concretely, if an email has the text:\n",
    "#\n",
    "#                  The quick brown fox jumped over the lazy dog.\n",
    "#\n",
    "#               Then, the word_indices vector for this text might look \n",
    "#               like:\n",
    "#               \n",
    "#                   60  100   33   44   10     53  60  58   5\n",
    "#\n",
    "#               where, we have mapped each word onto a number, for example:\n",
    "#\n",
    "#                   the   -- 60\n",
    "#                   quick -- 100\n",
    "#                   ...\n",
    "#\n",
    "#              (note: the above numbers are just an example and are not the\n",
    "#               actual mappings).\n",
    "#\n",
    "#              Your task is take one such word_indices vector and construct\n",
    "#              a binary feature vector that indicates whether a particular\n",
    "#              word occurs in the email. That is, x(i) = 1 when word i\n",
    "#              is present in the email. Concretely, if the word 'the' (say,\n",
    "#              index 60) appears in the email, then x(60) = 1. The feature\n",
    "#              vector should look like:\n",
    "#\n",
    "#              x = [ 0 0 0 0 1 0 0 0 ... 0 0 0 0 1 ... 0 0 0 1 0 ..];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting features from sample email (emailSample1.txt)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nExtracting features from sample email (emailSample1.txt)\\n');\n",
    "\n",
    "# Extract Features\n",
    "features = emailFeatures(word_indices);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of feature vector:  1899\n",
      "Number of non-zero entries:  45\n"
     ]
    }
   ],
   "source": [
    "# Print Stats\n",
    "print('Length of feature vector: ', len(features));\n",
    "print('Number of non-zero entries: ', np.sum(features > 0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3: Train Linear SVM for Spam Classification \n",
    "In this section, you will train a linear classifier to determine if an email is Spam or Not-Spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 1899)\n",
      "(4000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load the Spam Email dataset\n",
    "# You will have X, y in your environment\n",
    "\n",
    "spamTrain=loadmat('spamTrain.mat')\n",
    "spamTrain.keys()\n",
    "X=spamTrain['X']\n",
    "y=spamTrain['y']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import numpy as np\n",
    "\n",
    "def svmTrain(X, y, C, kernelFunction, tol=1e-3, max_passes=-1, sigma=0.1):\n",
    "    \"\"\"Trains an SVM classifier\"\"\"\n",
    "\n",
    "    y = y.flatten() # prevents warning\n",
    "\n",
    "    # alternative to emulate mapping of 0 -> -1 in svmTrain.m\n",
    "    #  but results are identical without it\n",
    "    # also need to cast from unsigned int to regular int\n",
    "    # otherwise, contour() in visualizeBoundary.py doesn't work as expected\n",
    "    # y = y.astype(\"int32\")\n",
    "    # y[y==0] = -1\n",
    "\n",
    "    if kernelFunction == \"gaussian\":\n",
    "        clf = svm.SVC(C = C, kernel=\"precomputed\", tol=tol, max_iter=max_passes, verbose=2)\n",
    "        return clf.fit(gaussianKernelGramMatrix(X,X, sigma=sigma), y)\n",
    "\n",
    "    # elif kernelFunction == \"linear\":\n",
    "    #     clf = svm.SVC(C = C, kernel=\"precomputed\", tol=tol, max_iter=max_passes, verbose=2)\n",
    "    #     return clf.fit(np.dot(X,X.T).T, y)\n",
    "\n",
    "    else: # works with \"linear\", \"rbf\"\n",
    "        clf = svm.SVC(C = C, kernel=kernelFunction, tol=tol, max_iter=max_passes, verbose=2)\n",
    "        return clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Linear SVM (Spam Classification)\n",
      "\n",
      "(this may take 1 to 2 minutes) ...\n",
      "\n",
      "[LibSVM]Training Accuracy:  99.825\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining Linear SVM (Spam Classification)\\n')\n",
    "print('(this may take 1 to 2 minutes) ...\\n')\n",
    "\n",
    "C = 0.1;\n",
    "model = svmTrain(X, y, C, 'linear');\n",
    "p_train = model.predict(X);\n",
    "print('Training Accuracy: ', np.mean(p.flatten() == y.flatten()) * 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spamTest=loadmat('spamTest.mat')\n",
    "spamTest.keys()\n",
    "Xtest=spamTest['Xtest']\n",
    "ytest=spamTest['ytest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  98.9\n"
     ]
    }
   ],
   "source": [
    "p_test=model.predict(Xtest)\n",
    "print('Test Accuracy: ', np.mean(p_test.flatten() == ytest.flatten()) * 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Top Predictors of Spam\n",
    "\n",
    "Since the model we are training is a linear SVM, we can inspect the weights learned by the model to understand better how it is determining whether an email is spam or not. The following code finds the words with the highest weights in the classifier. Informally, the classifier 'thinks' that these words are the most likely indicators of spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = model.coef_[0]\n",
    "\n",
    "# from http://stackoverflow.com/a/16486305/583834\n",
    "# reverse sorting by index\n",
    "indices = w.argsort()[::-1][:15]\n",
    "vocabList = sorted(getVocabList().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top predictors of spam: \n",
      "\n",
      " our (0.500614) \n",
      " click (0.465916) \n",
      " remov (0.422869) \n",
      " guarante (0.383622) \n",
      " visit (0.367710) \n",
      " basenumb (0.345064) \n",
      " dollar (0.323632) \n",
      " will (0.269724) \n",
      " price (0.267298) \n",
      " pleas (0.261169) \n",
      " most (0.257298) \n",
      " nbsp (0.253941) \n",
      " lo (0.253467) \n",
      " ga (0.248297) \n",
      " hour (0.246404) \n"
     ]
    }
   ],
   "source": [
    "print('\\nTop predictors of spam: \\n');\n",
    "for idx in indices: \n",
    "    print( ' {:s} ({:f}) '.format( vocabList[idx], float(w[idx]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort the weights and obtin the vocabulary list\n",
    "[weight, idx] = sort(model.w, 'descend');\n",
    "vocabList = getVocabList();\n",
    "\n",
    "fprintf('\\nTop predictors of spam: \\n');\n",
    "for i = 1:15\n",
    "    fprintf(' %-15s (%f) \\n', vocabList{idx(i)}, weight(i));\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Try Your Own Emails \n",
    "\n",
    "Now that you've trained the spam classifier, you can use it on your own emails! In the starter code, we have included spamSample1.txt,\n",
    "spamSample2.txt, emailSample1.txt and emailSample2.txt as examples. \n",
    "The following code reads in one of these emails and then uses your \n",
    "learned SVM classifier to determine whether the email is Spam or \n",
    "Not Spam\n",
    "\n",
    "Set the file to be read in (change this to spamSample2.txt,\n",
    "emailSample1.txt or emailSample2.txt to see different predictions on\n",
    "different emails types). Try your own emails as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Processed Email ====\n",
      "\n",
      "\n",
      "first\n",
      "releas\n",
      "sold\n",
      "out\n",
      "new\n",
      "singlefamili\n",
      "home\n",
      "releas\n",
      "in\n",
      "the\n",
      "cupertino\n",
      "union\n",
      "\n",
      "school\n",
      "district\n",
      "resid\n",
      "number\n",
      "releas\n",
      "for\n",
      "sale\n",
      "with\n",
      "quick\n",
      "close\n",
      "price\n",
      "start\n",
      "\n",
      "from\n",
      "dollarnumb\n",
      "number\n",
      "number\n",
      "with\n",
      "the\n",
      "recent\n",
      "success\n",
      "and\n",
      "sell\n",
      "out\n",
      "of\n",
      "our\n",
      "\n",
      "first\n",
      "releas\n",
      "cupertino\n",
      "live\n",
      "work\n",
      "announc\n",
      "the\n",
      "highlyanticip\n",
      "releas\n",
      "of\n",
      "resid\n",
      "\n",
      "number\n",
      "cupertino\n",
      "is\n",
      "at\n",
      "the\n",
      "heart\n",
      "of\n",
      "innov\n",
      "so\n",
      "it\n",
      "s\n",
      "no\n",
      "coincid\n",
      "it\n",
      "s\n",
      "where\n",
      "you\n",
      "\n",
      "ll\n",
      "find\n",
      "cupertino\n",
      "live\n",
      "work\n",
      "a\n",
      "modern\n",
      "collect\n",
      "as\n",
      "uniqu\n",
      "as\n",
      "the\n",
      "address\n",
      "it\n",
      "call\n",
      "\n",
      "home\n",
      "each\n",
      "home\n",
      "ha\n",
      "been\n",
      "design\n",
      "with\n",
      "it\n",
      "own\n",
      "custom\n",
      "premium\n",
      "finish\n",
      "even\n",
      "more\n",
      "\n",
      "extraordinari\n",
      "is\n",
      "the\n",
      "bonu\n",
      "separ\n",
      "suit\n",
      "you\n",
      "will\n",
      "find\n",
      "with\n",
      "each\n",
      "of\n",
      "these\n",
      "home\n",
      "\n",
      "the\n",
      "possibl\n",
      "are\n",
      "endless\n",
      "for\n",
      "thi\n",
      "uniqu\n",
      "separ\n",
      "suit\n",
      "creativ\n",
      "work\n",
      "space\n",
      "a\n",
      "bonu\n",
      "\n",
      "famili\n",
      "room\n",
      "entertain\n",
      "space\n",
      "librari\n",
      "think\n",
      "tank\n",
      "becaus\n",
      "thi\n",
      "is\n",
      "a\n",
      "one\n",
      "of\n",
      "a\n",
      "kind\n",
      "\n",
      "opportun\n",
      "in\n",
      "the\n",
      "highlysought\n",
      "after\n",
      "cupertino\n",
      "union\n",
      "school\n",
      "district\n",
      "don\n",
      "t\n",
      "let\n",
      "\n",
      "someon\n",
      "els\n",
      "beat\n",
      "you\n",
      "to\n",
      "the\n",
      "idea\n",
      "to\n",
      "call\n",
      "thi\n",
      "home\n",
      "open\n",
      "hous\n",
      "saturdaysunday\n",
      "\n",
      "number\n",
      "numbernumb\n",
      "numberpm\n",
      "or\n",
      "call\n",
      "list\n",
      "agent\n",
      "for\n",
      "privat\n",
      "show\n",
      "prequalifi\n",
      "\n",
      "cupertino\n",
      "union\n",
      "school\n",
      "district\n",
      "monta\n",
      "vista\n",
      "high\n",
      "school\n",
      "number\n",
      "bedroom\n",
      "number\n",
      "\n",
      "number\n",
      "bath\n",
      "detach\n",
      "bonu\n",
      "suit\n",
      "up\n",
      "to\n",
      "number\n",
      "number\n",
      "sq\n",
      "ft\n",
      "with\n",
      "number\n",
      "car\n",
      "park\n",
      "\n",
      "bonu\n",
      "suit\n",
      "includ\n",
      "full\n",
      "bath\n",
      "kitchenett\n",
      "custom\n",
      "marbl\n",
      "or\n",
      "granit\n",
      "slab\n",
      "counter\n",
      "in\n",
      "\n",
      "the\n",
      "gourmet\n",
      "kitchen\n",
      "and\n",
      "builtin\n",
      "premium\n",
      "ge\n",
      "applianc\n",
      "\n",
      "\n",
      "=========================\n",
      "\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "email=readFile('myemail.txt');\n",
    "word_indices  = processEmail(email);\n",
    "features = emailFeatures(word_indices);\n",
    "p1=model.predict(features.T)\n",
    "print(p1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
